# Task ID: 001
# Title: Create Full Database Backup
# Status: done
# Dependencies: none
# Priority: high
# Description: Create comprehensive backup of entire database before starting any refactoring work. This backup will be used for rollback if needed.

# Details:
Create comprehensive backup of entire database before starting any refactoring work. This backup will be used for rollback if needed.

## Implementation Steps:
1. Check current database size: `SELECT pg_size_pretty(pg_database_size('database_name'));`
2. Create database dump: `pg_dump database_name > backup_$(date +%Y%m%d_%H%M%S).sql`
3. Compress backup: `gzip backup_*.sql`
4. Upload to cloud storage (AWS S3, Google Cloud, etc.)
5. Keep local copy in safe location
6. Document backup location in BACKUP_LOG.md
7. Test restore on staging/test environment

## Acceptance Criteria:
- PostgreSQL/MySQL database dump created successfully
- Backup file is verified (can be restored)
- Backup is stored in at least 2 locations (local + cloud)
- Backup file is dated with current timestamp
- Backup size is reasonable (matches expected database size)
- Test restore on a separate instance to verify integrity

## Files Modified:
- `Database dump file: backup_YYYYMMDD_HHMMSS.sql.gz`
- `BACKUP_LOG.md (create if doesn't exist)`

## Important Notes:
- Ensure backup includes all tables, schemas, and data
- Backup should be taken during low-traffic period
- Coordinate with team to minimize user activity during backup
- Estimate: 30-60 minutes depending on database size

## ðŸ“Š Analysis:

**Context for Dev Repo:**
Since this is a DEVELOPMENT repository (separated from production), database backup is primarily for rollback during refactoring, not for disaster recovery.

**Complexity: LOW**
- Standard PostgreSQL/MySQL operation
- Well-documented procedures
- Low risk of failure

**Why This Task Matters:**
1. **Safety Net**: Allows complete rollback if refactoring goes wrong
2. **Baseline**: Captures the "before" state for comparison
3. **Testing**: Provides data for staging environment
4. **Confidence**: Development team can refactor aggressively knowing data is safe

**Parallel Execution Opportunity:**
âœ… **CAN RUN IN PARALLEL** with tasks 002, 003, 005, 006
âš ï¸ **BLOCKS** task 004 (staging needs this backup to restore)

## ðŸ’¡ Recommendations:

**For Dev Environment:**
1. **Skip "Low-Traffic" Requirement** - This is dev, not production. Backup anytime.
2. **Automate**: Create a bash script for repeatable backups:
   ```bash
   #!/bin/bash
   # backup_db.sh
   DATE=$(date +%Y%m%d_%H%M%S)
   pg_dump madaris_dev > backup_${DATE}.sql
   gzip backup_${DATE}.sql
   aws s3 cp backup_${DATE}.sql.gz s3://madaris-backups/
   echo "Backup created: backup_${DATE}.sql.gz" >> BACKUP_LOG.md
   ```

3. **Keep Multiple Versions**: Since disk space is cheap, keep 3-5 backups:
   - Before refactoring start
   - After each major phase (Phase 1, 3, 6, etc.)
   - Before major migrations

4. **Optimize for Speed**: Use parallel dump for large databases:
   ```bash
   pg_dump -j 4 -Fd madaris_dev -f backup_directory/
   ```

5. **Verify Immediately**: Don't skip the restore test. It's the ONLY way to know backup is valid:
   ```bash
   createdb madaris_test_restore
   gunzip -c backup_*.sql.gz | psql madaris_test_restore
   psql madaris_test_restore -c "SELECT COUNT(*) FROM chapters_chapter;"
   dropdb madaris_test_restore
   ```

**Risk Mitigation:**
- âš ï¸ **Gotcha**: pg_dump doesn't include roles/users. Backup separately:
  ```bash
  pg_dumpall --roles-only > backup_roles_${DATE}.sql
  ```

- âš ï¸ **Gotcha**: Large databases may take hours. Check size first:
  ```sql
  SELECT pg_size_pretty(pg_database_size('madaris_dev'));
  ```

- âš ï¸ **Gotcha**: Ensure destination has 2-3x database size free space

**Time-Saving Tip:**
If database is >10GB, consider using `pg_basebackup` instead of `pg_dump` for faster backups.

**Parallel Agent Assignment:**
Assign to **Agent Type 6: DevOps Specialist** - they handle infrastructure tasks efficiently.

# Test Strategy:
1. Verify backup file exists and has reasonable size
2. Restore on test database to verify integrity
3. Confirm all tables present in restored database
4. Compare row counts with original database